---
sidebar_position: 2
title: Who is this for?
---

# Who is this for?

## Primary audience

**Red teamers who test AI model behavior.** You craft adversarial prompts, evaluate safety guardrails, and document vulnerabilities. You already have a process. This gives you structured methods that plug into that process and surface findings you'd miss otherwise.

**AI safety teams.** You need repeatable methodology for model behavior testing across releases, across models, and across team members. Ad hoc testing doesn't scale. Structured ideation and documented attack journeys do.

**Team leads organizing adversarial testing.** You need to coordinate red teaming work across multiple people, ensure coverage, and communicate findings to stakeholders. This provides shared vocabulary, templates, and reporting structure.

## What you'll get from this

- **Better attack coverage.** Persona-driven testing forces you out of your default mental model of "attacker" and into perspectives that generate different findings.
- **Reproducible attacks.** Journey-mapped multi-turn sequences can be handed to another tester and replicated exactly. Your findings aren't locked in one person's head.
- **More creative attack vectors.** Structured ideation methods systematically explore the space of possible attacks instead of cycling through known techniques.
- **A lens for finding where to probe.** Norman's Gulfs give you a formal way to identify gaps between what an AI system is supposed to do and what it actually allows.
- **Findings that get acted on.** Harm-centered reporting communicates impact in terms stakeholders understand, not just technical severity scores.

## On technique

This site teaches the process for approaching adversarial testing systematically. For specific prompting techniques, resources like the [Prompting Guide](https://www.promptingguide.ai/) cover that territory. This is the approach. Technique references teach the moves.
